{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Mining API query testing\n",
    "\n",
    "Load the target datafile, and see how the query results work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload local package definitions for each cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/grequests.py:22: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.contrib.pyopenssl (/usr/local/lib/python3.8/site-packages/urllib3/contrib/pyopenssl.py)']. \n",
      "  curious_george.patch_all(thread=False, select=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import requests\n",
    "import grequests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from arg_mine import DATA_DIR\n",
    "from arg_mine.data.loaders import get_gdelt_df\n",
    "from arg_mine.api import classify, auth, session, errors\n",
    "from arg_mine import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env variables to get the API key\n",
    "user_id, api_key = auth.load_auth_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>headline_image_url</th>\n",
       "      <th>content_url</th>\n",
       "      <th>labeled_argument</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200115111500</td>\n",
       "      <td>Liberal MPs back Science Minister Karen Andrew...</td>\n",
       "      <td>https://static.ffx.io/images/$zoom_0.2627%2C$m...</td>\n",
       "      <td>https://www.smh.com.au/politics/federal/libera...</td>\n",
       "      <td>the science in her interview with The Age and...</td>\n",
       "      <td>2020-01-15 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200117184500</td>\n",
       "      <td>Several crowd-pullers on day two of KLF</td>\n",
       "      <td>https://www.thehindu.com/news/cities/kozhikode...</td>\n",
       "      <td>https://www.thehindu.com/news/cities/kozhikode...</td>\n",
       "      <td>Guha, who talked about patriotism and jingois...</td>\n",
       "      <td>2020-01-17 18:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200106233000</td>\n",
       "      <td>Seven arrested after gas rig protest</td>\n",
       "      <td>https://www.stourbridgenews.co.uk/resources/im...</td>\n",
       "      <td>https://www.stourbridgenews.co.uk/news/nationa...</td>\n",
       "      <td>three demands for the Scottish and UK Governm...</td>\n",
       "      <td>2020-01-06 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200101111500</td>\n",
       "      <td>Australia sending aid to wildfire towns as dea...</td>\n",
       "      <td>https://bloximages.newyork1.vip.townnews.com/h...</td>\n",
       "      <td>https://www.heraldmailmedia.com/news/nation/au...</td>\n",
       "      <td>this season the worst on record and reignited ...</td>\n",
       "      <td>2020-01-01 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200107101500</td>\n",
       "      <td>A hot, dry country caught between fire and a c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://global.chinadaily.com.cn/a/202001/07/WS...</td>\n",
       "      <td>, which is burned to generate electricity, wit...</td>\n",
       "      <td>2020-01-07 10:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         datetime                                              title  \\\n",
       "0  20200115111500  Liberal MPs back Science Minister Karen Andrew...   \n",
       "1  20200117184500            Several crowd-pullers on day two of KLF   \n",
       "2  20200106233000               Seven arrested after gas rig protest   \n",
       "3  20200101111500  Australia sending aid to wildfire towns as dea...   \n",
       "4  20200107101500  A hot, dry country caught between fire and a c...   \n",
       "\n",
       "                                  headline_image_url  \\\n",
       "0  https://static.ffx.io/images/$zoom_0.2627%2C$m...   \n",
       "1  https://www.thehindu.com/news/cities/kozhikode...   \n",
       "2  https://www.stourbridgenews.co.uk/resources/im...   \n",
       "3  https://bloximages.newyork1.vip.townnews.com/h...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         content_url  \\\n",
       "0  https://www.smh.com.au/politics/federal/libera...   \n",
       "1  https://www.thehindu.com/news/cities/kozhikode...   \n",
       "2  https://www.stourbridgenews.co.uk/news/nationa...   \n",
       "3  https://www.heraldmailmedia.com/news/nation/au...   \n",
       "4  http://global.chinadaily.com.cn/a/202001/07/WS...   \n",
       "\n",
       "                                    labeled_argument           timestamp  \n",
       "0   the science in her interview with The Age and... 2020-01-15 11:15:00  \n",
       "1   Guha, who talked about patriotism and jingois... 2020-01-17 18:45:00  \n",
       "2   three demands for the Scottish and UK Governm... 2020-01-06 23:30:00  \n",
       "3  this season the worst on record and reignited ... 2020-01-01 11:15:00  \n",
       "4  , which is burned to generate electricity, wit... 2020-01-07 10:15:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_datapath = os.path.join(DATA_DIR, \"raw\", \"2020-climate-change-narrative\")\n",
    "csv_filepath = os.path.join(csv_datapath, \"WebNewsEnglishSnippets.2020.csv\")\n",
    "\n",
    "url_df = get_gdelt_df(csv_filepath)\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the `classify` request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_df.iloc[0].content_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_BASE_URL = \"https://api.argumentsearch.com/en/classify\"\n",
    "timeout = 5\n",
    "\n",
    "topic = \"climate change\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"topicRelevance\": \"word2vec\",\n",
    "    \"predictStance\": True,\n",
    "    \"computeAttention\": True,\n",
    "    \"showOnlyArguments\": False,\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    CLASSIFY_BASE_URL,\n",
    "    json=payload,\n",
    "    timeout=timeout,\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata', 'sentences'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = response.json()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a single URL\n",
    "Get the doc and sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = classify.classify_url_sentences(topic, url_df.content_url.values[0], user_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifiedSentence(url='https://www.smh.com.au/politics/federal/liberals-speak-out-to-back-science-minister-on-climate-change-action-20200115-p53rs1.html', doc_id='657f9dd95eb97597e34d0c05b5a93ba6', topic='climate change', sentence_id='da903878c62343fb482bfad67a7523f1', argument_confidence=0.9836708698421717, argument_label='argument', sentence_original='Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.', sentence_preprocessed='Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.', sort_confidence=0.9907711706754092, stance_confidence=0.9978714715086467, stance_label='pro')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.ClassifiedSentence.from_dict(url, topic, response['sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argumentConfidence': 0.9836708698421717,\n",
       " 'argumentLabel': 'argument',\n",
       " 'sentenceOriginal': 'Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.',\n",
       " 'sentencePreprocessed': 'Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.',\n",
       " 'sortConfidence': 0.9907711706754092,\n",
       " 'stanceConfidence': 0.9978714715086467,\n",
       " 'stanceLabel': 'pro'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator.\n"
     ]
    }
   ],
   "source": [
    "# good example of a link that is no longer valid\n",
    "url = url_df.content_url.values[3]\n",
    "try: \n",
    "    response = classify.classify_url_sentences(topic, url, user_id, api_key)\n",
    "except errors.Refused as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a batch of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108459,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.content_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:2020-06-30 19:45:30,986:arg_mine.api.classify: Attempting url 1 of 20\n",
      "DEBUG:2020-06-30 19:45:32,843:arg_mine.api.classify: Attempting url 2 of 20\n",
      "DEBUG:2020-06-30 19:45:33,776:arg_mine.api.classify: Attempting url 3 of 20\n",
      "DEBUG:2020-06-30 19:45:35,229:arg_mine.api.classify: Attempting url 4 of 20\n",
      "WARNING:2020-06-30 19:45:35,797:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.heraldmailmedia.com/news/nation/australia-sending-aid-to-wildfire-towns-as-death-toll-rises/article_883fa793-6c0a-547e-8f77-b5964f1d7182.html\n",
      "DEBUG:2020-06-30 19:45:35,798:arg_mine.api.classify: Attempting url 5 of 20\n",
      "DEBUG:2020-06-30 19:45:37,159:arg_mine.api.classify: Attempting url 6 of 20\n",
      "DEBUG:2020-06-30 19:45:38,425:arg_mine.api.classify: Attempting url 7 of 20\n",
      "WARNING:2020-06-30 19:45:38,708:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.seattlepi.com/news/world/article/Cooler-weather-brings-respite-in-Australian-14950114.php\n",
      "DEBUG:2020-06-30 19:45:38,709:arg_mine.api.classify: Attempting url 8 of 20\n",
      "WARNING:2020-06-30 19:45:39,825:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.airdrietoday.com/national-business/starbucks-goals-for-sustainability-will-require-significant-consumer-buy-in-2037947\n",
      "DEBUG:2020-06-30 19:45:39,826:arg_mine.api.classify: Attempting url 9 of 20\n",
      "WARNING:2020-06-30 19:45:41,007:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.heraldbulletin.com/news/nation_world/davos-chief-welcomes-views-of-trump-greta-thunberg-at-forum/article_24dbb151-7132-5c48-a7df-c13b67004ec0.html\n",
      "DEBUG:2020-06-30 19:45:41,009:arg_mine.api.classify: Attempting url 10 of 20\n",
      "DEBUG:2020-06-30 19:45:42,292:arg_mine.api.classify: Attempting url 11 of 20\n",
      "DEBUG:2020-06-30 19:45:43,312:arg_mine.api.classify: Attempting url 12 of 20\n",
      "DEBUG:2020-06-30 19:45:44,439:arg_mine.api.classify: Attempting url 13 of 20\n",
      "DEBUG:2020-06-30 19:45:45,519:arg_mine.api.classify: Attempting url 14 of 20\n",
      "WARNING:2020-06-30 19:45:46,181:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://ravallirepublic.com/news/local/article_cb291465-d430-5477-8b41-1bedda9beaea.html\n",
      "DEBUG:2020-06-30 19:45:46,182:arg_mine.api.classify: Attempting url 15 of 20\n",
      "WARNING:2020-06-30 19:45:47,310:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.caledonianrecord.com/opinion/columns/shawn-shouldice-small-business-owners-prepare-to-dodge-bullets-during/article_9f1b6399-7ca9-59bc-8788-b4d05ec5825c.html\n",
      "DEBUG:2020-06-30 19:45:47,311:arg_mine.api.classify: Attempting url 16 of 20\n",
      "DEBUG:2020-06-30 19:45:48,758:arg_mine.api.classify: Attempting url 17 of 20\n",
      "WARNING:2020-06-30 19:45:49,427:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.nsnews.com/trudeau-meets-pallister-and-the-meng-hearing-in-the-news-for-jan-20-1.24056357\n",
      "DEBUG:2020-06-30 19:45:49,429:arg_mine.api.classify: Attempting url 18 of 20\n",
      "DEBUG:2020-06-30 19:45:51,821:arg_mine.api.classify: Attempting url 19 of 20\n",
      "DEBUG:2020-06-30 19:45:53,050:arg_mine.api.classify: Attempting url 20 of 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration took 24.9 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc_list, sentence_list, refused_doc_list = classify.collect_sentences_by_topic(topic, url_df.content_url.values[:20])\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refused_doc_list))\n",
    "refused_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try concurrent requests with grequest\n",
    "\n",
    "`grequest` does concurrent threaded requests, but has memory issues for long lists. We can chunk the async requests and write the outputs to storage to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = classify.fetch_concurrent(topic, url_list = url_df.content_url.values[:20], chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df, sentences_df, missing_urls = classify.process_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the sentences as GT arguments\n",
    "The given snippit contains context surrounding the given identified key phrase. Tokenizing the phrase will not work.\n",
    "It is still unclear on whether or not the GT data can be claimed to be an argument; likely not.\n",
    "\n",
    "> This final dataset covers worldwide English language online news coverage 2015-2020 mentioning \"climate change\" OR \"global warming\" OR \"climate crisis\" OR \"greenhouse gas\" OR \"greenhouse gases\" OR \"carbon tax\" totaling 6.3 million articles. [...]  \n",
    ">Most importantly, for each match, a short snippet is shown that shows the first instance of one of the climate change phrases above in the article with the 100 characters before and after the appearance, truncated to the nearest word (if the 100th character before or after the phrase appears in the middle of a word, the window will be shrunk to the closest full word). Note that in the majority of cases the first match in the article is selected, but sometimes due to the nature of the finite automaton used to generate the snippets, a later match may be chosen from the article if it allows for a larger context window under certain circumstances.  \n",
    ">Using a window of 100 characters before and after the match allows for brief non-consumptive snippets that show the context of the match and allow a better understanding of whether the article's mention of climate change was a cursory mention or central to the story and the argument, evidence  and context of the narrative within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the extracted sentences\n",
    "target_dir = \"gdelt-climate-change-docs\"\n",
    "in_data_path = os.path.join(DATA_DIR, \"processed\", target_dir)\n",
    "\n",
    "docs_filename = \"gdelt_2020_docs.csv\"\n",
    "sentences_filename = \"gdelt_2020_sentences.csv\"\n",
    "\n",
    "# load the files into dataframes\n",
    "docs_df = pd.read_csv(os.path.join(in_data_path, docs_filename))\n",
    "sentences_df = pd.read_csv(os.path.join(in_data_path, sentences_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing this out to use for parsing later\n",
    "keyword_list = [\n",
    "    \"climate change\",\n",
    "    \"global warming\",\n",
    "    \"climate crisis\",\n",
    "    \"greenhouse gas\",\n",
    "    \"greenhouse gases\",\n",
    "    \"carbon tax\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility methods for finding the GT argument sentences we extract\n",
    "\n",
    "def match_doc_id(url, docs_df):\n",
    "    \"\"\"Get the document id from the ground truth dataset\"\"\"\n",
    "    return docs_df[url == docs_df['url']]['doc_id'].iloc[0]\n",
    "\n",
    "def get_doc_sentences(doc_id, sentences_df):\n",
    "    \"\"\"Return all sentences for a given doc_id\"\"\"\n",
    "    return sentences_df[sentences_df.doc_id == doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = match_doc_id(url_df.iloc[0]['content_url'], docs_df)\n",
    "doc_sentences = get_doc_sentences(doc_id, sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df['has_labeled_arg'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: sentence_id, dtype: object)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-4a02c2a7f743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msentences_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentences_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_labeled_arg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "content_url = url_df.iloc[0]\n",
    "snippit = url_df['labeled_argument'][0]\n",
    "doc_id = match_doc_id(content_url['content_url'], docs_df)\n",
    "doc_sentences = get_doc_sentences(doc_id, sentences_df)\n",
    "\n",
    "# tokenize the GT argument\n",
    "arg_tokens = snippit.split(\".\") if isinstance(snippit, str) else None\n",
    "arg_tokens = [s.strip() for s in arg_tokens]\n",
    "\n",
    "for token in arg_tokens:\n",
    "    matches = doc_sentences[doc_sentences.sentence_original.str.contains(token, na=False)]['sentence_id']\n",
    "    print(matches)\n",
    "\n",
    "    sentences_df[sentences_df['sentence_id'] == matches.values[0]]['has_labeled_arg'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the science in her interview with The Age and The Sydney Morning Herald on Wednesday'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>argument_confidence</th>\n",
       "      <th>argument_label</th>\n",
       "      <th>sentence_original</th>\n",
       "      <th>sentence_preprocessed</th>\n",
       "      <th>sort_confidence</th>\n",
       "      <th>stance_confidence</th>\n",
       "      <th>stance_label</th>\n",
       "      <th>has_labeled_arg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [url, doc_id, topic, sentence_id, argument_confidence, argument_label, sentence_original, sentence_preprocessed, sort_confidence, stance_confidence, stance_label, has_labeled_arg]\n",
       "Index: []"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sentences[doc_sentences.sentence_original.str.contains(token, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17     True\n",
       "18    False\n",
       "19    False\n",
       "20    False\n",
       "21    False\n",
       "22    False\n",
       "24    False\n",
       "25    False\n",
       "27    False\n",
       "28    False\n",
       "29    False\n",
       "30    False\n",
       "31    False\n",
       "32    False\n",
       "33    False\n",
       "34    False\n",
       "35    False\n",
       "36    False\n",
       "Name: sentence_original, dtype: bool"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sentences.sentence_original.str.contains(\"Liberal backbenchers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"Australians want us to get on with the job of meeting our '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' the science in her interview with The Age and The Sydney Morning Herald on Wednesday',\n",
       " ' \"Overwhelmingly, Australians accept the science of climate change and they now have lived experience of the climatic conditions this summer,\" he said',\n",
       " ' \"Australians want us to get on with the job of meeting our ']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17    False\n",
       "18    False\n",
       "19    False\n",
       "20    False\n",
       "21    False\n",
       "22    False\n",
       "24    False\n",
       "25    False\n",
       "27    False\n",
       "28    False\n",
       "29    False\n",
       "30    False\n",
       "31    False\n",
       "32    False\n",
       "33    False\n",
       "34    False\n",
       "35    False\n",
       "36    False\n",
       "Name: has_labeled_arg, dtype: bool"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.loc[sentences_df.doc_id == doc_id]['has_labeled_arg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -r 1 -n 1\n",
    "# this takes forever!! why does it take so long?\n",
    "\n",
    "# can we find the ground truth argument sentences for a given doc?\n",
    "sentences_df['has_labeled_arg'] = False\n",
    "\n",
    "content_url = url_df.iloc[0]\n",
    "# for content_url in url_df['content_url']:\n",
    "# get the doc sentences\n",
    "\n",
    "doc_id = match_doc_id(content_url['content_url'], docs_df)\n",
    "doc_sentences = get_doc_sentences(doc_id, sentences_df)\n",
    "\n",
    "# tokenize the GT argument\n",
    "arg_tokens = [sentence.split(\".\") if isinstance(sentence, str) else None for sentence in url_df['labeled_argument'] ]\n",
    "\n",
    "# if arg_tokens is None:\n",
    "#     continue\n",
    "# then iterate over sentences to see if the arg is contained in the sentence\n",
    "for token in arg_tokens:\n",
    "    try:\n",
    "        sentences_df.loc[sentences_df.doc_id == doc_id, 'has_labeled_arg'] = (\n",
    "            sentences_df.loc[sentences_df.doc_id == doc_id, 'has_labeled_arg'] ^ \n",
    "            doc_sentences.sentence_original.str.contains(token, na=False)\n",
    "        )\n",
    "    except TypeError as e:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(arg_tokens==None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check timing of different approaches\n",
    "\n",
    "Does grequests give us a performance boost?\n",
    "* time serial extraction vs using grequests\n",
    "\n",
    "Does returning all sentences vs just arguments give us a performance hit?\n",
    "* time extraction of 50 articles to see if API times are significantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc_list, sentence_list, refused_doc_list = classify.collect_sentences_by_topic(topic, url_df.content_url.values[:num_docs])\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "responses = classify.fetch_concurrent(topic, url_list = url_df.content_url.values[:num_docs], chunk_size=10)\n",
    "docs_df, sentences_df, missing_urls = classify.process_responses(responses)\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
