{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Mining API query testing\n",
    "\n",
    "Load the target datafile, and see how the query results work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload local package definitions for each cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/grequests.py:22: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.contrib.pyopenssl (/usr/local/lib/python3.8/site-packages/urllib3/contrib/pyopenssl.py)']. \n",
      "  curious_george.patch_all(thread=False, select=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import requests\n",
    "import grequests\n",
    "import pandas as pd\n",
    "\n",
    "from arg_mine import DATA_DIR\n",
    "from arg_mine.data.loaders import get_gdelt_df\n",
    "from arg_mine.api import classify, auth, session, errors\n",
    "from arg_mine import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env variables to get the API key\n",
    "user_id, api_key = auth.load_auth_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_datapath = os.path.join(DATA_DIR, \"raw\", \"2020-climate-change-narrative\")\n",
    "csv_filepath = os.path.join(csv_datapath, \"WebNewsEnglishSnippets.2020.csv\")\n",
    "\n",
    "url_df = get_gdelt_df(csv_filepath)\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the `classify` request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_df.iloc[0].content_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_BASE_URL = \"https://api.argumentsearch.com/en/classify\"\n",
    "timeout = 5\n",
    "\n",
    "topic = \"climate change\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"topicRelevance\": \"word2vec\",\n",
    "    \"predictStance\": True,\n",
    "    \"computeAttention\": True,\n",
    "    \"showOnlyArguments\": False,\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    CLASSIFY_BASE_URL,\n",
    "    json=payload,\n",
    "    timeout=timeout,\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = response.json()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = classify.classify_url_sentences(topic, url_df.content_url.values[0], user_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify.ClassifiedSentence.from_dict(url, topic, response['sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good example of a link that is no longer valid\n",
    "url = url_df.content_url.values[3]\n",
    "try: \n",
    "    response = classify.classify_url_sentences(topic, url, user_id, api_key)\n",
    "except errors.Refused as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a batch of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.content_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc_list, sentence_list, refused_doc_list = classify.collect_sentences_by_topic(topic, url_df.content_url.values[:20])\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refused_doc_list))\n",
    "refused_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try concurrent requests with grequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pool_size=5\n",
    "chunk_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = url_df.content_url.values[:20]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "s = session.get_session(pool_size=pool_size)\n",
    "full_list = []\n",
    "\n",
    "# for i in range(0, len(url_list), chunk_size):\n",
    "print(\">>>> iteration: {}\".format(i))\n",
    "chunk_urls = url_list[i:i + chunk_size]\n",
    "unsent_requests = (\n",
    "    grequests.post(\n",
    "        session.ApiUrl.CLASSIFY_BASE_URL,\n",
    "        json=classify.bundle_payload(topic, u),\n",
    "        session=s,\n",
    "        allow_redirects=False,\n",
    "        )\n",
    "    for u in chunk_urls\n",
    ")\n",
    "output = grequests.map(unsent_requests, size=pool_size) #, exception_handler=classify.exception_handler)\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect data for unit tests\n",
    "Using tools in the package, run queries that give known responses, and save those responses as json fixtures for unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import json\n",
    "from tests.fixtures import save_json_request_fixture, load_json_fixture\n",
    "import logging\n",
    "\n",
    "logger = logging.basicConfig()\n",
    "\n",
    "SAVE_FIXTURES = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url  = url_df.content_url.values[0]\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"model\": \"default\",\n",
    "    \"topicRelevance\": classify.TopicRelevance.WORD2VEC,\n",
    "    \"predictStance\": True,  # we don't want to predict stance without context\n",
    "    \"computeAttention\": False,  # doesnt work for BERT-based models (the default model)\n",
    "    \"showOnlyArguments\": True,  # only return sentences classified as arguments\n",
    "    \"userMetadata\": url,\n",
    "}\n",
    "\n",
    "result = session.fetch(session.ApiUrl.CLASSIFY_BASE_URL, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_FIXTURES:\n",
    "    save_json_request_fixture(\"response_classify_only_args.json\", payload, result, status_code=200, drop_keys=['apiKey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### known bad article request, gives 404 on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this url gives a known 404 on the article server\n",
    "url = url_df.content_url.values[3]\n",
    "result = None\n",
    "print(url)\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"model\": \"default\",\n",
    "    \"topicRelevance\": classify.TopicRelevance.WORD2VEC,\n",
    "    \"predictStance\": True,  # we don't want to predict stance without context\n",
    "    \"computeAttention\": False,  # doesnt work for BERT-based models (the default model)\n",
    "    \"showOnlyArguments\": True,  # only return sentences classified as arguments\n",
    "    \"userMetadata\": url,\n",
    "}\n",
    "\n",
    "try: \n",
    "    result = session.fetch(session.ApiUrl.CLASSIFY_BASE_URL, payload)\n",
    "except errors.Refused as e:\n",
    "    print(result)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(session.ApiUrl.CLASSIFY_BASE_URL, json=payload)\n",
    "print(response.json())\n",
    "error_response = response.json()\n",
    "if SAVE_FIXTURES:\n",
    "    save_json_request_fixture(\"response_classify_refused_remote_404.json\", payload, error_response, status_code=400, drop_keys=['apiKey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad parameters test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good url\n",
    "url = url_df.content_url.values[0]\n",
    "result = None\n",
    "print(url)\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"model\": \"i_am_a_ bad_model\",\n",
    "    \"userMetadata\": url,\n",
    "}\n",
    "try:\n",
    "    result = session.fetch(session.ApiUrl.CLASSIFY_BASE_URL, payload)\n",
    "except errors.InternalGatewayError as e:\n",
    "    print(result)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(session.ApiUrl.CLASSIFY_BASE_URL, json=payload)\n",
    "if SAVE_FIXTURES:\n",
    "    save_json_request_fixture(\"response_classify_500_bad_payload.json\", payload, \"\", status_code=response.status_code, drop_keys=['apiKey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timeout test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_df.content_url.values[0]\n",
    "result = None\n",
    "print(url)\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"model\": \"default\",\n",
    "    \"topicRelevance\": classify.TopicRelevance.WORD2VEC,\n",
    "    \"predictStance\": True,  # we don't want to predict stance without context\n",
    "    \"computeAttention\": False,  # doesnt work for BERT-based models (the default model)\n",
    "    \"showOnlyArguments\": True,  # only return sentences classified as arguments\n",
    "    \"userMetadata\": url,\n",
    "}\n",
    "try:\n",
    "    result = session.fetch(session.ApiUrl.CLASSIFY_BASE_URL, payload, timeout=0.1)\n",
    "except errors.NotResponding as e:\n",
    "    print(result)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.post(session.ApiUrl.CLASSIFY_BASE_URL, json=payload, timeout=0.1)\n",
    "except requests.Timeout as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
