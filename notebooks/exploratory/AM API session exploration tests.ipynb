{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument Mining API query testing\n",
    "\n",
    "Load the target datafile, and see how the query results work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload local package definitions for each cell\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/grequests.py:22: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.contrib.pyopenssl (/usr/local/lib/python3.8/site-packages/urllib3/contrib/pyopenssl.py)']. \n",
      "  curious_george.patch_all(thread=False, select=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import requests\n",
    "import grequests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from arg_mine import DATA_DIR\n",
    "from arg_mine.api import classify, auth, session, errors\n",
    "from arg_mine import utils\n",
    "from arg_mine.data import labelers, loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env variables to get the API key\n",
    "user_id, api_key = auth.load_auth_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2020-07-06 23:56:58,290:arg_mine.data.loaders: reading data from: /opt/workspace/data/raw/2020-climate-change-narrative/WebNewsEnglishSnippets.2020.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>headline_image_url</th>\n",
       "      <th>content_url</th>\n",
       "      <th>topic_context</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20200115111500</td>\n",
       "      <td>Liberal MPs back Science Minister Karen Andrew...</td>\n",
       "      <td>https://static.ffx.io/images/$zoom_0.2627%2C$m...</td>\n",
       "      <td>https://www.smh.com.au/politics/federal/libera...</td>\n",
       "      <td>the science in her interview with The Age and...</td>\n",
       "      <td>2020-01-15 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20200117184500</td>\n",
       "      <td>Several crowd-pullers on day two of KLF</td>\n",
       "      <td>https://www.thehindu.com/news/cities/kozhikode...</td>\n",
       "      <td>https://www.thehindu.com/news/cities/kozhikode...</td>\n",
       "      <td>Guha, who talked about patriotism and jingois...</td>\n",
       "      <td>2020-01-17 18:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20200106233000</td>\n",
       "      <td>Seven arrested after gas rig protest</td>\n",
       "      <td>https://www.stourbridgenews.co.uk/resources/im...</td>\n",
       "      <td>https://www.stourbridgenews.co.uk/news/nationa...</td>\n",
       "      <td>three demands for the Scottish and UK Governm...</td>\n",
       "      <td>2020-01-06 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20200101111500</td>\n",
       "      <td>Australia sending aid to wildfire towns as dea...</td>\n",
       "      <td>https://bloximages.newyork1.vip.townnews.com/h...</td>\n",
       "      <td>https://www.heraldmailmedia.com/news/nation/au...</td>\n",
       "      <td>this season the worst on record and reignited ...</td>\n",
       "      <td>2020-01-01 11:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20200107101500</td>\n",
       "      <td>A hot, dry country caught between fire and a c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://global.chinadaily.com.cn/a/202001/07/WS...</td>\n",
       "      <td>, which is burned to generate electricity, wit...</td>\n",
       "      <td>2020-01-07 10:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         datetime                                              title  \\\n",
       "0  20200115111500  Liberal MPs back Science Minister Karen Andrew...   \n",
       "1  20200117184500            Several crowd-pullers on day two of KLF   \n",
       "2  20200106233000               Seven arrested after gas rig protest   \n",
       "3  20200101111500  Australia sending aid to wildfire towns as dea...   \n",
       "4  20200107101500  A hot, dry country caught between fire and a c...   \n",
       "\n",
       "                                  headline_image_url  \\\n",
       "0  https://static.ffx.io/images/$zoom_0.2627%2C$m...   \n",
       "1  https://www.thehindu.com/news/cities/kozhikode...   \n",
       "2  https://www.stourbridgenews.co.uk/resources/im...   \n",
       "3  https://bloximages.newyork1.vip.townnews.com/h...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         content_url  \\\n",
       "0  https://www.smh.com.au/politics/federal/libera...   \n",
       "1  https://www.thehindu.com/news/cities/kozhikode...   \n",
       "2  https://www.stourbridgenews.co.uk/news/nationa...   \n",
       "3  https://www.heraldmailmedia.com/news/nation/au...   \n",
       "4  http://global.chinadaily.com.cn/a/202001/07/WS...   \n",
       "\n",
       "                                       topic_context           timestamp  \n",
       "0   the science in her interview with The Age and... 2020-01-15 11:15:00  \n",
       "1   Guha, who talked about patriotism and jingois... 2020-01-17 18:45:00  \n",
       "2   three demands for the Scottish and UK Governm... 2020-01-06 23:30:00  \n",
       "3  this season the worst on record and reignited ... 2020-01-01 11:15:00  \n",
       "4  , which is burned to generate electricity, wit... 2020-01-07 10:15:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_datapath = os.path.join(DATA_DIR, \"raw\", \"2020-climate-change-narrative\")\n",
    "csv_filepath = os.path.join(csv_datapath, \"WebNewsEnglishSnippets.2020.csv\")\n",
    "\n",
    "url_df = loaders.get_gdelt_df(csv_filepath)\n",
    "url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the `classify` request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = url_df.iloc[0].content_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFY_BASE_URL = \"https://api.argumentsearch.com/en/classify\"\n",
    "timeout = 5\n",
    "\n",
    "topic = \"climate change\"\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"topic\": topic,\n",
    "    \"userID\": user_id,\n",
    "    \"apiKey\": api_key,\n",
    "    \"targetUrl\": url,\n",
    "    \"topicRelevance\": \"word2vec\",\n",
    "    \"predictStance\": True,\n",
    "    \"computeAttention\": True,\n",
    "    \"showOnlyArguments\": False,\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    CLASSIFY_BASE_URL,\n",
    "    json=payload,\n",
    "    timeout=timeout,\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata', 'sentences'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = response.json()\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try a single URL\n",
    "Get the doc and sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = classify.classify_url_sentences(topic, url_df.content_url.values[0], user_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifiedSentence(url='https://www.smh.com.au/politics/federal/liberals-speak-out-to-back-science-minister-on-climate-change-action-20200115-p53rs1.html', doc_id='657f9dd95eb97597e34d0c05b5a93ba6', topic='climate change', sentence_id='da903878c62343fb482bfad67a7523f1', argument_confidence=0.9836708698421717, argument_label='argument', sentence_original='Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.', sentence_preprocessed='Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.', sort_confidence=None, stance_confidence=0.9978714715086467, stance_label='pro')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify.ClassifiedSentence.from_dict(url, topic, response['sentences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'argumentConfidence': 0.9836708698421717,\n",
       " 'argumentLabel': 'argument',\n",
       " 'sentenceOriginal': 'Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.',\n",
       " 'sentencePreprocessed': 'Mr Morrison said Ms Andrews had \"well set out\" the government policy and signalled again that he would bring forward future policies to adapt to a changing climate while also doing more to reduce greenhouse gas emissions.',\n",
       " 'stanceConfidence': 0.9978714715086467,\n",
       " 'stanceLabel': 'pro'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator.\n"
     ]
    }
   ],
   "source": [
    "# good example of a link that is no longer valid\n",
    "url = url_df.content_url.values[3]\n",
    "try: \n",
    "    response = classify.classify_url_sentences(topic, url, user_id, api_key)\n",
    "except errors.Refused as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a batch of urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108459,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_df.content_url.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:2020-07-07 00:05:37,048:arg_mine.api.classify: Attempting url 1 of 20\n",
      "DEBUG:2020-07-07 00:05:38,572:arg_mine.api.classify: Attempting url 2 of 20\n",
      "DEBUG:2020-07-07 00:05:39,410:arg_mine.api.classify: Attempting url 3 of 20\n",
      "DEBUG:2020-07-07 00:05:40,905:arg_mine.api.classify: Attempting url 4 of 20\n",
      "WARNING:2020-07-07 00:05:41,494:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.heraldmailmedia.com/news/nation/australia-sending-aid-to-wildfire-towns-as-death-toll-rises/article_883fa793-6c0a-547e-8f77-b5964f1d7182.html\n",
      "DEBUG:2020-07-07 00:05:41,495:arg_mine.api.classify: Attempting url 5 of 20\n",
      "DEBUG:2020-07-07 00:05:43,384:arg_mine.api.classify: Attempting url 6 of 20\n",
      "DEBUG:2020-07-07 00:05:44,629:arg_mine.api.classify: Attempting url 7 of 20\n",
      "WARNING:2020-07-07 00:05:44,965:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.seattlepi.com/news/world/article/Cooler-weather-brings-respite-in-Australian-14950114.php\n",
      "DEBUG:2020-07-07 00:05:44,967:arg_mine.api.classify: Attempting url 8 of 20\n",
      "WARNING:2020-07-07 00:05:46,013:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.airdrietoday.com/national-business/starbucks-goals-for-sustainability-will-require-significant-consumer-buy-in-2037947\n",
      "DEBUG:2020-07-07 00:05:46,014:arg_mine.api.classify: Attempting url 9 of 20\n",
      "WARNING:2020-07-07 00:05:47,178:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.heraldbulletin.com/news/nation_world/davos-chief-welcomes-views-of-trump-greta-thunberg-at-forum/article_24dbb151-7132-5c48-a7df-c13b67004ec0.html\n",
      "DEBUG:2020-07-07 00:05:47,179:arg_mine.api.classify: Attempting url 10 of 20\n",
      "DEBUG:2020-07-07 00:05:48,472:arg_mine.api.classify: Attempting url 11 of 20\n",
      "DEBUG:2020-07-07 00:05:49,418:arg_mine.api.classify: Attempting url 12 of 20\n",
      "DEBUG:2020-07-07 00:05:50,448:arg_mine.api.classify: Attempting url 13 of 20\n",
      "DEBUG:2020-07-07 00:05:51,553:arg_mine.api.classify: Attempting url 14 of 20\n",
      "WARNING:2020-07-07 00:05:52,214:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://ravallirepublic.com/news/local/article_cb291465-d430-5477-8b41-1bedda9beaea.html\n",
      "DEBUG:2020-07-07 00:05:52,218:arg_mine.api.classify: Attempting url 15 of 20\n",
      "WARNING:2020-07-07 00:05:53,491:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.caledonianrecord.com/opinion/columns/shawn-shouldice-small-business-owners-prepare-to-dodge-bullets-during/article_9f1b6399-7ca9-59bc-8788-b4d05ec5825c.html\n",
      "DEBUG:2020-07-07 00:05:53,493:arg_mine.api.classify: Attempting url 16 of 20\n",
      "DEBUG:2020-07-07 00:05:54,799:arg_mine.api.classify: Attempting url 17 of 20\n",
      "WARNING:2020-07-07 00:05:55,444:arg_mine.api.classify: Refused: Refused: 400: Website could not be crawled or returned an empty result. Please contact an administrator., url=https://www.nsnews.com/trudeau-meets-pallister-and-the-meng-hearing-in-the-news-for-jan-20-1.24056357\n",
      "DEBUG:2020-07-07 00:05:55,446:arg_mine.api.classify: Attempting url 18 of 20\n",
      "DEBUG:2020-07-07 00:05:58,343:arg_mine.api.classify: Attempting url 19 of 20\n",
      "DEBUG:2020-07-07 00:05:59,562:arg_mine.api.classify: Attempting url 20 of 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration took 24.7 s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc_list, sentence_list, refused_doc_list = classify.collect_sentences_by_topic(topic, url_df.content_url.values[:20])\n",
    "\n",
    "print(\"iteration took {:.3} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.heraldmailmedia.com/news/nation/australia-sending-aid-to-wildfire-towns-as-death-toll-rises/article_883fa793-6c0a-547e-8f77-b5964f1d7182.html',\n",
       " 'https://www.seattlepi.com/news/world/article/Cooler-weather-brings-respite-in-Australian-14950114.php',\n",
       " 'https://www.airdrietoday.com/national-business/starbucks-goals-for-sustainability-will-require-significant-consumer-buy-in-2037947',\n",
       " 'https://www.heraldbulletin.com/news/nation_world/davos-chief-welcomes-views-of-trump-greta-thunberg-at-forum/article_24dbb151-7132-5c48-a7df-c13b67004ec0.html',\n",
       " 'https://ravallirepublic.com/news/local/article_cb291465-d430-5477-8b41-1bedda9beaea.html',\n",
       " 'https://www.caledonianrecord.com/opinion/columns/shawn-shouldice-small-business-owners-prepare-to-dodge-bullets-during/article_9f1b6399-7ca9-59bc-8788-b4d05ec5825c.html',\n",
       " 'https://www.nsnews.com/trudeau-meets-pallister-and-the-meng-hearing-in-the-news-for-jan-20-1.24056357']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(refused_doc_list))\n",
    "refused_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try concurrent requests with grequest\n",
    "\n",
    "`grequest` does concurrent threaded requests, but has memory issues for long lists. We can chunk the async requests and write the outputs to storage to avoid this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:2020-07-07 00:06:01,867:arg_mine.api.classify: >>>> starting doc extraction\n",
      "DEBUG:2020-07-07 00:06:13,361:arg_mine.api.classify: iteration 0 took 11.488 s (10 docs)\n",
      "DEBUG:2020-07-07 00:06:27,932:arg_mine.api.classify: iteration 1 took 14.569 s (10 docs)\n",
      "DEBUG:2020-07-07 00:06:27,933:arg_mine.api.classify: 20 URLs took 26.067 s\n"
     ]
    }
   ],
   "source": [
    "responses = classify.fetch_concurrent(topic, url_list = url_df.content_url.values[:20], chunk_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:2020-07-07 00:06:27,977:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,980:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,981:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,982:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,988:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,990:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n",
      "DEBUG:2020-07-07 00:06:27,993:arg_mine.api.classify: 400 : {'error': 'Website could not be crawled or returned an empty result. Please contact an administrator.'}\n"
     ]
    }
   ],
   "source": [
    "docs_df, sentences_df, missing_urls = classify.process_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312, 11)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the sentences as GT arguments\n",
    "The given snippit contains context surrounding the given identified key phrase. Tokenizing the phrase will not work.\n",
    "It is still unclear on whether or not the GT data can be claimed to be an argument; likely not.\n",
    "\n",
    "> This final dataset covers worldwide English language online news coverage 2015-2020 mentioning \"climate change\" OR \"global warming\" OR \"climate crisis\" OR \"greenhouse gas\" OR \"greenhouse gases\" OR \"carbon tax\" totaling 6.3 million articles. [...]  \n",
    ">Most importantly, for each match, a short snippet is shown that shows the first instance of one of the climate change phrases above in the article with the 100 characters before and after the appearance, truncated to the nearest word (if the 100th character before or after the phrase appears in the middle of a word, the window will be shrunk to the closest full word). Note that in the majority of cases the first match in the article is selected, but sometimes due to the nature of the finite automaton used to generate the snippets, a later match may be chosen from the article if it allows for a larger context window under certain circumstances.  \n",
    ">Using a window of 100 characters before and after the match allows for brief non-consumptive snippets that show the context of the match and allow a better understanding of whether the article's mention of climate change was a cursory mention or central to the story and the argument, evidence  and context of the narrative within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the extracted sentences\n",
    "target_dir = \"gdelt-climate-change-docs\"\n",
    "in_data_path = os.path.join(DATA_DIR, \"processed\", target_dir)\n",
    "\n",
    "docs_filename = \"gdelt_2020_docs.csv\"\n",
    "sentences_filename = \"gdelt_2020_sentences.csv\"\n",
    "\n",
    "# load the files into dataframes\n",
    "docs_df = pd.read_csv(os.path.join(in_data_path, docs_filename))\n",
    "sentences_df = pd.read_csv(os.path.join(in_data_path, sentences_filename))\n",
    "sentences_df.dropna(subset=['sentence_original'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing this out to use for parsing later\n",
    "keyword_list = [\n",
    "    \"climate change\",\n",
    "    \"global warming\",\n",
    "    \"climate crisis\",\n",
    "    \"greenhouse gas\",\n",
    "    \"greenhouse gases\",\n",
    "    \"carbon tax\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = labelers.match_doc_id(url_df.iloc[0]['content_url'], docs_df)\n",
    "doc_sentences = labelers.get_doc_sentences(doc_id, sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labeled_argument'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labeled_argument'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-98da4d57be91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcontent_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msnippit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labeled_argument'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_doc_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_url\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdoc_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabelers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_doc_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'labeled_argument'"
     ]
    }
   ],
   "source": [
    "# manually try the labeling algorithm:\n",
    "\n",
    "sentences_df['has_labeled_arg'] = False\n",
    "\n",
    "sentences_df.columns\n",
    "\n",
    "\n",
    "content_url = url_df.iloc[0]\n",
    "snippit = url_df['labeled_argument'][0]\n",
    "doc_id = labelers.match_doc_id(content_url['content_url'], docs_df)\n",
    "doc_sentences = labelers.get_doc_sentences(doc_id, sentences_df)\n",
    "\n",
    "# tokenize the GT argument\n",
    "arg_tokens = snippit.split(\".\") if isinstance(snippit, str) else None\n",
    "arg_tokens = [s.strip() for s in arg_tokens]\n",
    "\n",
    "for token in arg_tokens:\n",
    "    matches = doc_sentences[doc_sentences.sentence_original.str.contains(token, na=False)]['sentence_id']\n",
    "    print(matches)\n",
    "    # only look at the first match\n",
    "    sentences_df.loc[sentences_df['sentence_id'] == matches.values[0], 'has_labeled_arg'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df.loc[sentences_df.doc_id == doc_id]['has_labeled_arg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df_crop = url_df[url_df['content_url'].isin(docs_df.url.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelers.label_doc_sentences_with_context(url_df_crop.iloc[0], docs_df, sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_hash = utils.unique_hash(\"https://www.theglobeandmail.com/featured-reports/article-whats-the-right-university-for-you/\")\n",
    "doc_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df[docs_df.doc_id == doc_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelers.get_doc_sentences(doc_hash, sentences_df)['sentence_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences.sentence_original.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sentences_df = labelers.label_gdelt_context(url_df, docs_df, sentences_df)\n",
    "\n",
    "\n",
    "print(\"labeling took {:0.2f} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \", science, sustainable design engineering and veterinary medicine.\"\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences = labelers.get_doc_sentences('716931eb57b75ab7161622f9e2d03f6d', sentences_df)\n",
    "np.where(doc_sentences.sentence_original.str.contains(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences.sentence_original.iloc[72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences.iloc[477]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences.iloc[477].sentence_original.str.contains(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sentences.sentence_original.iloc[475:479].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check timing of different approaches\n",
    "\n",
    "Does grequests give us a performance boost?\n",
    "* time serial extraction vs using grequests\n",
    "\n",
    "Does returning all sentences vs just arguments give us a performance hit?\n",
    "* time extraction of 50 articles to see if API times are significantly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "doc_list, sentence_list, refused_doc_list = classify.collect_sentences_by_topic(topic, url_df.content_url.values[:num_docs])\n",
    "\n",
    "print(\"iteration took {:.2f} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "responses = classify.fetch_concurrent(topic, url_list = url_df.content_url.values[:num_docs], chunk_size=10)\n",
    "docs_df, sentences_df, missing_urls = classify.process_responses(responses)\n",
    "\n",
    "print(\"iteration took {:.2f} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
